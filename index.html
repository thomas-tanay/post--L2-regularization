<!doctype html><html>
<head>
<meta charset="utf-8">
<script src="http://distill.pub/template.v1.js"></script>

<script src="https://d3js.org/d3.v4.min.js"></script>

<script src="https://d3js.org/d3-color.v1.min.js"></script>
<script src="https://d3js.org/d3-format.v1.min.js"></script>
<script src="https://d3js.org/d3-interpolate.v1.min.js"></script>
<script src="https://d3js.org/d3-scale.v1.min.js"></script>
<script src="https://d3js.org/d3-scale-chromatic.v1.min.js"></script>

<script src="https://d3js.org/d3-collection.v1.min.js"></script>
<script src="https://d3js.org/d3-dispatch.v1.min.js"></script>
<script src="https://d3js.org/d3-dsv.v1.min.js"></script>
<script src="https://d3js.org/d3-request.v1.min.js"></script>
<script src="https://d3js.org/d3-queue.v3.min.js"></script>
<script src="https://d3js.org/d3-path.v1.min.js"></script>
<script src="https://d3js.org/d3-dispatch.v1.min.js"></script>
<script src="https://d3js.org/d3-selection.v1.min.js"></script>
<script src="https://d3js.org/d3-drag.v1.min.js"></script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjs/3.12.1/math.min.js"> </script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true
    },
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>
<script type="text/javascript"
   src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

<script type="text/front-matter">
  title: "A New Angle on L2 Regularization"
  description: "Adversarial Examples in Linear Classification and Beyond"
  authors:
  - Thomas Tanay: http://thomastanay.com/
  - Lewis D Griffin: https://sites.google.com/site/lewisdgriffin/home
  affiliations:
  - CoMPLEX, UCL: http://www.ucl.ac.uk/complex
  - CoMPLEX, UCL: http://www.ucl.ac.uk/complex
</script>

<script src="assets/vis_functions.js"></script>

</head>

<body>
<dt-article class="centered">
  <h1 class="l-middle">A New Angle on L2 Regularization</h1>
  
  <style>
  #abstract {
    display: -webkit-flex;
    display: flex;
	flex-direction: column;
	margin-top: 50px;
	margin-bottom: 30px;
  }
  #abstract-fig {
	width: 816px;
	height: 233px;
	border-style: solid;
	border-width: 1.5px;
	border-color: rgb(60%,60%,60%);
  }
  #abstract-rest {
    margin-top: 15px;
    display: -webkit-flex;
    display: flex;
	justify-content: space-between;	
  }
  #abstract-controler {
    display: -webkit-flex;
    display: flex;
	flex-direction: column;
	width: 190px;
	margin-left: auto;
	margin-right: auto;
  }
  #abstract-input {
    margin-left: 0px;
	margin-top: 18px;
	margin-bottom: 28px;
  }
  .abstract-text-content {
	font: 15px/1.55em "Roboto";
	line-height: 1.5em;
	color: rgb(60%,60%,60%);
  }
  #abstract-text {
    width: 540px;
	font: 15px/1.55em "Roboto";
	line-height: 1.5em;
	color: rgb(60%,60%,60%);
	margin-left: auto;
	margin-right: auto;
  }
  </style>
  
  <div class="l-middle">
  <div id="abstract">
    <div id="abstract-fig"> </div>
	<div id="abstract-rest">
	  <div id="abstract-controler">
	    <div class="abstract-text-content"> Regularization: </div>
        <input id="abstract-input" type="range" min="0" max="80" step="1"> </input>
		<div class="abstract-text-content"> <span id="abstract_err_train"> ... </span> </div>
		<div class="abstract-text-content"> <span id="abstract_adv_distance"> ... </span> </div>
	  </div>
	  <div id="abstract-text">
		In linear models and small neural nets, L2 regularization can be understood as a balancing mechanism between two objectives:
		minimizing the training error and maximizing the distance between the data and the boundary. <br>
		<br>
		In practice, using an appropriate level of regularization helps avoid overfitting and constitutes a good defence against adversarial attacks.	
	  </div> 
	</div>
  </div>
  <dt-byline></dt-byline>
  </div>
  
  <script>
    abstract_fig();
  </script>
  
  <p> Deep neural networks have been shown to be vulnerable to the adversarial example phenomenon: all models tested so far can have their classifications dramatically 
  altered by small image perturbations <dt-cite key="szegedy2013intriguing,goodfellow2014explaining"></dt-cite>. The following predictions were for instance made by
  a state-of-the-art network trained to recognize celebrities <dt-cite key="parkhi2015deep"></dt-cite>: <p>
  
  <div id="Carell-Deschanel"></div>
  
  <script>
    Carell_Deschanel();
  </script>
  
  <p> This result is puzzling for two reasons. First, it challenges conventional wisdom on generalization in machine learning.
  Second, it constitutes a potential threat to real-world applications <dt-cite key="papernot2016practical,kurakin2016adversarial"></dt-cite>. Researchers at 
  OpenAI have for instance recently created images that <a href="https://blog.openai.com/robust-adversarial-inputs/"> reliably fool neural network classifiers 
  when viewed from varied scales and perspectives</a>. Understanding this phenomenon and improving deep networks' robustness has thus become an important 
  research objective.</p>
  
  <p>Several approaches have been explored already. The phenomenon has been described in detail <dt-cite key="moosavi2016deepfool,carlini2016towards"></dt-cite> 
  and some theoretical analysis has been provided <dt-cite key="bastani2016measuring,fawzi2016robustness"></dt-cite>. Attempts have been made at designing more robust 
  architectures <dt-cite key="gu2014towards,papernot2016distillation,zhao2016suppressing"></dt-cite> or at detecting adversarial examples during evaluation 
  <dt-cite key="bhagoji2017dimensionality,feinman2017detecting,grosse2017statistical,metzen2017detecting"></dt-cite>. <em>Adversarial training</em> has also been 
  introduced as a new regularization technique penalising adversarial directions <dt-cite key="goodfellow2014explaining,shaham2015understanding,miyato2017virtual,sankaranarayanan2017regularizing"></dt-cite>.
  Despite these numerous contributions, the problem stays largely unresolved <dt-cite key="goodfellow2017attacking,carlini2017adversarial"></dt-cite>.
  Confronted with this difficulty, we propose to proceed from fundamentals: focusing on linear classification first and then increasing complexity incrementally.</p>

  <hr>
  
  <h2> A Toy Problem </h2>
  
  <p> In linear classification, adversarial perturbations are often understood as a property of the dot product in high dimension. A widespread intuition is that:
  "for high dimensional problems, we can make many infinitesimal changes to the input that add up to one large change to the output" 
  <dt-cite key="goodfellow2014explaining"></dt-cite>.
  Here, we challenge this intuition and argue instead that adversarial examples exist when the classification boundary lies close the data manifold&mdash;independently 
  of the image space dimension. </p>
  
  <p> Let's start with a minimal toy problem: a two-dimensional image space where each image is a function of $a$ and $b$. </p>
  
  <div id="toy-problem1">
  </div>
  
  <script>
    toy_problem1();
  </script>

  <p> In this simple image space, we define two classes of images that we will soon try to separate with a linear classifier. </p>
  
  <div id="toy-problem2">
  </div>
  
  <script>
    toy_problem2();
  </script>

  <p> The two classes just defined can be separated by an infinite number of linear classifiers. Consider for instance the following line $\mathscr{L}_{\theta}$: </p>
  
  <div id="toy-problem3">
  </div>
  
  <script>
    toy_problem3();
  </script>
  
  <style>
  .projected-mirror {
    display: -webkit-flex;
    display: flex;
	justify-content: flex-start;
	align-items: center;
  }
  .projected-mirror-input {
	width: 130px;
	height: 130px;
  }
  .projected-mirror-text {
    display: -webkit-flex;
    display: flex;
	flex-direction: column;
	align-items: flex-start;
    margin-left: 40px;
	margin-top: 20px;
	margin-bottom: 20px;
  }
  .projected-mirror-def {
    font-size: 17px;
    margin-bottom: 0px;
  }
  .projected-mirror-label {
    margin-top: -10px;
	margin-bottom: 20px;
    font: 15px "Roboto";
	color: rgb(60%,60%,60%);
  }
  </style>
  
  <p> This raises a first question. If all the linear classifiers $\mathscr{L}_{\theta}$ perform equally well on clean data,
    are they all equally robust to image perturbations? </p>
  
  <h4> Projected and mirror images </h4>
  
  <p> Consider an image $\boldsymbol{x}$ in class $I$. The closest image classified in the opposite class is <em>the projected image</em> of $\boldsymbol{x}$
  on $\mathscr{L}_{\theta}$: </p>
  <div class="l-body projected-mirror">
    <div class="projected-mirror-input" id="projected-mirror-input1"> </div>
	<div class="projected-mirror-text">
	  <div class="projected-mirror-def"> $$\boldsymbol{x}_p := \boldsymbol{x} - (\boldsymbol{x} \!\cdot\! \boldsymbol{w_{\theta}}) \; \boldsymbol{w_{\theta}}$$ </div>
	  <div class="projected-mirror-label"> Low confidence adversarial example </div>
	</div>
  </div>
  <p> When $\boldsymbol{x}$ and $\boldsymbol{x}_p$ are very close to each other, we say that $\boldsymbol{x}_p$ is an <em>adversarial example</em> of $\boldsymbol{x}$. 
  Observe though that $\boldsymbol{x}_p$ is classified with a <br>low confidence score (it lies on the boundary) and it is perhaps more interesting to 
  consider <em>high-confidence adversarial examples</em> <dt-cite key="carlini2017adversarial"></dt-cite>. <br>
  In the following, we focus on the <em>mirror image</em> of $\boldsymbol{x}$ through $\mathscr{L}_{\theta}$: </p>
  <div class="l-body projected-mirror">
    <div class="projected-mirror-input" id="projected-mirror-input2"> </div>
	<div class="projected-mirror-text">
	  <div class="projected-mirror-def"> $$\boldsymbol{x}_m := \boldsymbol{x} - 2\,(\boldsymbol{x} \!\cdot\! \boldsymbol{w_{\theta}}) \; \boldsymbol{w_{\theta}}$$ </div>
	  <div class="projected-mirror-label"> High confidence adversarial example </div>
	</div>
  </div>
  <p> By construction, $\boldsymbol{x}$ and $\boldsymbol{x}_m$ are at the same distance from the boundary and are classified with the same confidence level. </p>
  
  <script>
   projected_image();
   mirror_image();
  </script>
  
  <h4> A mirror image as a function of $\theta$ </h4>
  
  <p> Coming back to our original toy problem, we can now plot an image $\boldsymbol{x}$ and its mirror image $\boldsymbol{x}_m$ as a function of $\theta$. </p>
  
  <div id="toy-problem4">
  </div>
  
  <script>
    toy_problem4();
  </script>
  
  <p> We see that the distance between $\boldsymbol{x}$ and $\boldsymbol{x}_m$ varies. 
  The two borderline cases are of particular interest.</p>
  
  <style>
  #x-xm {
    margin-top: 40px;
	margin-bottom: 40px;
  }
  .horizontal-line {
    margin-top: 20px;
	margin-bottom: 20px;
  }
  .x-xm-div {
    display: -webkit-flex;
    display: flex;
	align-items: center;
	margin-right: 0px;
  }
  .x-xm-text {
    display: -webkit-flex;
    display: flex;
	flex-direction: column;
	margin-left: 20px;
  }
  .x-xm-text-header {
    font: 15px "Roboto";
	font-weight: bold;
  }
  .x-xm-text-subheader {
	margin-top: 10px;
	margin-bottom: 5px;
    font: 15px "Roboto";
	font-style: italic;
	color: rgb(60%,60%,60%);
  }
  .x-xm-text-content {
    width: 330px;
    font: 15px "Roboto";
	color: rgb(60%,60%,60%);
  }
  .x-xm-input {
    width: 250px;
	height: 130px;
	margin-left: auto;
	margin-right: auto;
  }
  </style>
  
  <div class="l-body" id="x-xm">
    <hr class="horizontal-line">
	<div class="x-xm-div">
	  <div class="x-xm-text">
	    <div class="x-xm-text-header"> When $\theta = 0$: </div>
		<div class="x-xm-text-subheader"> $\mathscr{L}_{\theta}$ does not suffer from adversarial examples. </div>
		<div class="x-xm-text-content">
		$\boldsymbol{x}$ is classified in $I$ with high confidence and <br>
		$\boldsymbol{x}_m$ is classified in $J$ with high confidence, <br>
		in agreement with human observers.
		</div>
	  </div>
	  <div class="x-xm-input" id="x-xm-small-theta"> </div>
	</div>
	<hr class="horizontal-line">
	<div class="x-xm-div">
	  <div class="x-xm-text">
	    <div class="x-xm-text-header"> When $\theta \to \pi/2$: </div>
		<div class="x-xm-text-subheader"> $\mathscr{L}_{\theta}$ suffers from strong adversarial examples. </div>
		<div class="x-xm-text-content">
		$\boldsymbol{x}$ is classified in $I$ with high confidence and <br>
		$\boldsymbol{x}_m$ is classified in $J$ with high confidence, <br>
		yet $\boldsymbol{x}_m$ is visually indistinguishable from $\boldsymbol{x}$.
		</div>
	  </div>
	  <div class="x-xm-input" id="x-xm-large-theta"> </div>
	</div>
	<hr class="horizontal-line">
  </div>
  
  <script>
   images_x1_xm1();
   images_x2_xm2();
  </script>
  
  <p> According to the new perspective, adversarial examples exist when the classification boundary approaches the data manifold in image space.
  This is what happens when $\theta$ tends to $\pi/2$: the line $\mathscr{L}_{\theta}$ tilts towards $I$ and $J$ and becomes increasingly 
  vulnerable to image perturbations. </p>
  
  <h4> The role played by L2 regularization </h4>
  
  <p> Adversarial examples exist when $\mathscr{L}_{\theta}$ is strongly tilted&mdash;but why would $\mathscr{L}_{\theta}$ behave in this way in practice? </p>
  
  <p> Our working hypothesis is that the classification boundary defined by standard linear learning algorithms such as Support Vector Machines (SVM) 
  or logistic regression tilts by overfitting noisy data points in the training set. 
  This hypothesis has a corrollary that can be easily tested: regularization techniques designed to reduce overfitting are expected 
  to mitigate the adversarial example phenomenon. </p>

  <style>
  #noisy-data {
    margin-top: 40px;
	margin-bottom: 40px;
  }
  .noisy-data-div {
    display: -webkit-flex;
    display: flex;
	align-items: center;
	margin-right: 0px;
  }
  .noisy-data-text {
    display: -webkit-flex;
    display: flex;
	flex-direction: column;
	margin-left: 20px;
  }
  .noisy-data-text-header {
    font: 15px "Roboto";
	font-weight: bold;
  }
  .noisy-data-text-subheader {
	margin-top: 10px;
	margin-bottom: 5px;
    font: 15px "Roboto";
	font-style: italic;
	color: rgb(60%,60%,60%);
  }
  .noisy-data-text-content {
    width: 400px;
    font: 15px "Roboto";
	color: rgb(60%,60%,60%);
  }
  .noisy-data-input {
    width: 130px;
    height: 130px;
	margin-left: auto;
	margin-right: auto;
  }
  #noisy-data-input0 {
	width: 280px;
	height: 150px;
	text-align:center;
	margin-bottom: 0px;
  }
  </style>
  
  <p> Consider for instance a training set containing one noisy data point $\boldsymbol{p}$. </p>
  <!--constituted of a large number of images from $I$ and $J$, plus the noisy data point $\boldsymbol{p}$ labelled in $I$:-->
  
  <div id="toy-problem5">
  </div>
  
  <script>
    toy_problem5();
  </script>

  <p> If we train an SVM or a logistic regression model on this training set, we observe two possible behaviours. </p>
  
  <div class="l-body" id="noisy-data">
    <hr class="horizontal-line">
	<div class="noisy-data-div">
	  <div class="noisy-data-text">
	    <div class="noisy-data-text-header"> Without L2 regularization: </div>
		<div class="noisy-data-text-subheader"> The classification boundary is strongly titled. </div>
		<div class="noisy-data-text-content"> 
        Most of the leeway available to fit the training data resides in the tilting angle of the boundary.
        Here, the data point $\boldsymbol{p}$ can be classified correctly, but the classifier obtained is then vulnerable to adversarial examples.
		</div>
	  </div>
	  <div class="noisy-data-input" id="noisy-data-input1"> </div>
	</div>
	<hr class="horizontal-line">
	<div class="noisy-data-div">
	  <div class="noisy-data-text">
	    <div class="noisy-data-text-header"> With L2 regularization: </div>
		<div class="noisy-data-text-subheader"> The classification boundary is not tilted. </div>
		<div class="noisy-data-text-content">
		L2 regularization reduces overfitting by allowing some training samples to be misclassified. 
        When enough regularization is used, the data point $\boldsymbol{p}$ is ignored and the classifier obtained is robust to adversarial examples.
		</div>
	  </div>
	  <div class="noisy-data-input" id="noisy-data-input2"> </div>
	</div>
	<hr class="horizontal-line">
  </div>
  
  <script>
  noisy_data1();
  noisy_data2();
  </script>

  <hr>
  
  <h2> In Linear Classification </h2>
  
  <p> At this point, one might legitimately wonder&mdash;what does a 1-dimensional data manifold lying in a 2-dimensional image space have to do with real world
  high-dimensional images? </p>
  
  <p> In the following, we show that the two main ideas introduced in the previous toy problem stay valid in the general case:
  adversarial examples exist when the classification boundary lies close to the data manifold and L2 regularization controls the tilting angle of the boundary.
  </p>
	
  <h3> Scaling the Loss Function </h3>
  
  <p> Let $I$ and $J$ be two classes of images and $\mathcal{C}$ a hyperplane boundary defining a linear classifier in $\mathbb{R}^d$. 
  $\mathcal{C}$ is specified by a normal weight vector $\boldsymbol{w}$ and a bias $b$. For an image $\boldsymbol{x}$ in $\mathbb{R}^d$, 
  we call <em> raw score </em> of $\boldsymbol{x}$ through $\mathcal{C}$ the value:
  $$s(\boldsymbol{x}) := \boldsymbol{w} \!\cdot\! \boldsymbol{x} + b \quad\quad\quad \boldsymbol{x} \text{ is classified in } \mathrel{\Bigg|}
  \begin{array}{@{}c@{}} \text{$I$ if } s(\boldsymbol{x}) \leq 0 \\ \text{$J$ if } s(\boldsymbol{x}) \geq 0 \end{array}$$</p>
  
  <p> Now, consider a training set $T$ of $n$ pairs $(\boldsymbol{x},y)$ where $\boldsymbol{x}$ is an image and 
  $y = \{ -1 \text{ if } \boldsymbol{x} \in I \;|\; 1 \text{ if } \boldsymbol{x} \in J\}$ is its label.
  For a given <em>loss function</em> $f$, the <em>empirical risk</em> on $T$ for the classifier $\mathcal{C}$ is:
  $$R(\boldsymbol{w},b) := \frac{1}{n}\sum_{(\boldsymbol{x},y) \in T}f\big(y\,s(\boldsymbol{x})\big)$$
  In general, learning a linear classifier consists of finding a weight vector $\boldsymbol{w}$ and a bias $b$ minimising $R(\boldsymbol{w},b)$ 
  for a well chosen loss function $f$.</p>
  
  <style>
  #small-bottom-margin {
    margin-bottom: 5px;
  }
  #small-top-margin {
    margin-top: 5px;
  }
  </style>
  
  <p id="small-bottom-margin"> Note that the raw score $s(\boldsymbol{x})$ can be seen as a scaled, signed distance. 
  If $d(\boldsymbol{x})$ is the actual <em> signed euclidean distance </em> between $\boldsymbol{x}$ and $\mathcal{C}$, we have:
  $$d(\boldsymbol{x}):= \boldsymbol{\hat{w}} \!\cdot\! \boldsymbol{x} + b^\prime \quad\quad\quad \text{where} \quad\quad \textstyle
  \boldsymbol{\hat{w}} := \frac{\boldsymbol{w}}{\lVert\boldsymbol{w}\rVert}\,,\quad b^\prime := \frac{b}{\lVert\boldsymbol{w}\rVert}$$
  $$\text{and} \quad s(\boldsymbol{x}) = \lVert\boldsymbol{w}\rVert\,d(\boldsymbol{x})$$
  Hence the norm $\lVert\boldsymbol{w}\rVert$ can be interpreted as a scaling parameter for the loss function. 
  The term $f\big(y\,s(\boldsymbol{x})\big)$ in the empirical risk can be written:
  $$f\big(y\,\lVert\boldsymbol{w}\rVert\,d(\boldsymbol{x})\big)$$
  and understood as: </p>
  
  <p id="small-top-margin"> <em>"$f$ applied to the signed euclidean distance $d(\boldsymbol{x})$, flipped left/right by the label $y = \pm 1$ and scaled by the parameter 
  $\lVert\boldsymbol{w}\rVert$"</em>. </p>
  
  <p>In binary classification, three notable loss functions are:</p>
  
  <style>
  .loss-functions {
	display: -webkit-flex;
    display: inline-flex;
	flex-direction: column;
	width: 100%;
  }
  .loss-functions-div {
    margin-bottom: 10px;
    display: -webkit-flex;
    display: flex;
  }
  .loss-functions-def {
    flex-grow: 1;
	display: -webkit-flex;
    display: flex;
	flex-direction: column;
  }
  .loss-functions-text {
    font: 18px -apple-system, BlinkMacSystemFont, "Roboto", sans-serif;
	font-weight: bold;
  }
  .loss-functions-equations {
    margin: auto;
  }
  .loss-functions-fig {
    margin-top: 0px;
    margin-left: auto;
	margin-right: 0px;
	width: 180px;
	height: 120px;
	border-style: solid;
	border-width: 1px;
	border-color: rgb(80%,80%,80%);
  }
  </style>
  
  <ul> 
  <li>
  <div class="loss-functions">
    <div class="loss-functions-div">
      <div class="loss-functions-def">
        <div class="loss-functions-text"> The 0-1 indicator function </div>
	    <div class="loss-functions-equations">
          $$f:z \rightarrow
          \begin{cases}
          1  \quad & \text{if } z \leq 0\\
          0        & \text{if } z \gt 0
          \end{cases}$$
	    </div>
      </div>
	  <div class="loss-functions-fig" id="indicator-01"> </div>
    </div>
    <div> With the 0-1 indicator function, the empirical risk $R(\boldsymbol{w},b)$ is simply the <em>error rate</em> on $T$.
    In a sense, this is the optimal loss function as minimizing the error rate is often the desired objective in practice. However, it cannot be 
    used with gradient descent (there is no gradient to descend: the devivative is null everywhere).</div>
  </div>
  </li>
  <li>
  <div class="loss-functions">
    <div class="loss-functions-div">
      <div class="loss-functions-def">
        <div class="loss-functions-text"> The hinge loss </div>
	    <div class="loss-functions-equations">
          $$f:z \rightarrow \max(1-z,0)$$
	    </div>
      </div>
	  <div class="loss-functions-fig" id="hinge-loss"> </div>
    </div>
  </div>
  <div> The hinge loss is used in Support Vector Machines (SVM). It penalises the misclassified data linearly while encouraging a safety margin of 1.</div> 
  </li>
  <li>
  <div class="loss-functions">
    <div class="loss-functions-div">
      <div class="loss-functions-def">
        <div class="loss-functions-text"> The softplus loss </div>
	    <div class="loss-functions-equations">
          $$f:z \rightarrow \ln\left(1+e^{-z}\right)$$
	    </div>
      </div>
	  <div class="loss-functions-fig" id="softplus-loss"> </div>
    </div>
  </div>
  <div> The softplus loss is used in logistic regression. It can be seen as a smooth version of the hinge loss. </div>
  </li>
  </ul>
  
  <script>
    loss_functions();
  </script>
  
  <p> The 0-1 indicator function is invariant to rescaling, but the hinge loss and the softplus loss are strongly affected: </p>
  
  <style>
  #fig3 {
    display: -webkit-flex;
    display: flex;
	flex-direction: column;
  }
  #fig3-controler {
    margin-bottom: 20px;
    display: -webkit-flex;
    display: flex;
	justify-content: center;
  }
  .fig3-caption {
    font-size: 15px;
  }
  #fig3-controler-input {
    margin-top: 7px;
    margin-left: 10px;
	margin-right: 10px;
  }
  #fig3-value-w {
    display: inline-block;
	text-align: right;
	width: 30px;
  }
  #fig3-content {
	margin-bottom: 10px;
    display: -webkit-flex;
    display: flex;
	justify-content: space-between;
	
  }
  .fig3-subfig {
    display: -webkit-flex;
    display: flex;
	flex-direction: column;
	align-items: center;
  }
  .fig3-subfig-content {
    margin-top: 10px;
	width: 240px;
	height: 160px;
	border-style: solid;
	border-width: 1px;
	border-color: rgb(80%,80%,80%);
  }
  </style>
  
  <figure class="l-middle">
    <div id="fig3">
      <div id="fig3-controler">
	    <div class="fig3-caption"> $\lVert\boldsymbol{w}\rVert:$ </div>
		<input id="fig3-controler-input" type="range" min="-1" max="2" step="0.01"> </input>
		<div class="fig3-caption"> $\displaystyle 10$ <sup> <span id="fig3-value-w">...</span> </sup> </div>
	  </div>
	  <div id="fig3-content">
	    <div class="fig3-subfig">
		  <div class="fig3-caption"> 0-1 indicator function </div>
          <div class="fig3-subfig-content" id="fig3-subfig1"> </div>
		</div>
	    <div class="fig3-subfig">
		  <div class="fig3-caption"> hinge loss </div>
          <div class="fig3-subfig-content" id="fig3-subfig2"> </div>
		</div>
	    <div class="fig3-subfig">
		  <div class="fig3-caption"> softplus loss </div>
          <div class="fig3-subfig-content" id="fig3-subfig3"> </div>
		</div>
	  </div>
	</div>
	<figcaption> Effect of the scaling parameter $\lVert\boldsymbol{w}\rVert$ on the three loss functions. The histograms represent the values of $d(\boldsymbol{x})$ 
	over the training data for a given $\boldsymbol{w}$ and $b$ and the full lines represent the values of $f$ (orange: class $I$, blue: class $J$).</figcaption>
  </figure>
  
  <script>
   fig3();
  </script>
  
  <p> Importantly, the hinge loss and the softplus loss behave in the same way for extreme values of the scaling parameter.</p>
  
  <style>
  .value-w {
    display: -webkit-flex;
    display: flex;
	align-items: center;
  }
  .value-w-div {
    margin-right: 30px;
  }
  .value-w-header {
    margin-top: 0px;
    font: 18px -apple-system, BlinkMacSystemFont, "Roboto", sans-serif;
	font-weight: bold;
  }
  .value-w-text {
    margin-bottom: 0px;
	font-style: italic;
  }
  .value-w-input {
	width: 180px;
	height: 120px;
	margin-right: 0px;
	border-style: solid;
	border-width: 1px;
	border-color: rgb(80%,80%,80%);
  }
  </style>
   
  <div class="l-body value-w">
    <div class="value-w-div">
      <p class="value-w-header"> When$\;\lVert\boldsymbol{w}\rVert\;$is large: </p>
      <p class="value-w-text"> The hinge loss and the softplus loss penalize only the misclassified data&mdash;and do so linearly. </p>
	</div>
	<div class="value-w-input" id="value-w-input1"> </div>
  </div>
  <p>  More precisely, both losses satisfy<dt-fn>
  <u>Hinge loss</u><br>
  \begin{align}
    \max(1-y\,\lVert\boldsymbol{w}\rVert\,d(\boldsymbol{x}),0)& \quad\> \;=\; \quad\> \textstyle\lVert\boldsymbol{w}\rVert \, \max\left(\lVert\boldsymbol{w}\rVert^{-1}-y\,d(\boldsymbol{x}),0\right)\\
    &\;\underset{\lVert\boldsymbol{w}\rVert \to +\infty}{\approx}\; \lVert\boldsymbol{w}\rVert \, \max\left(-y\,d(\boldsymbol{x}),0\right)
  \end{align}<br>
  
  <u>Softplus loss</u><br>
  \begin{align}
    \ln\left(1+e^{-y\,\lVert\boldsymbol{w}\rVert\,d(\boldsymbol{x})}\right) &\;\underset{\lVert\boldsymbol{w}\rVert \to +\infty}{\approx}\;
	  \begin{cases}
        -y\,\lVert\boldsymbol{w}\rVert\,d(\boldsymbol{x})  \quad & \text{if } y\,d(\boldsymbol{x}) \leq 0\\
        0                                                        & \text{if } y\,d(\boldsymbol{x}) \gt 0
      \end{cases}\\
	&\;\underset{\lVert\boldsymbol{w}\rVert \to +\infty}{\approx}\; \lVert\boldsymbol{w}\rVert \, \max\left(-y\,d(\boldsymbol{x}),0\right)
  \end{align}
  </dt-fn>:
  $$f\big(y\,\lVert\boldsymbol{w}\rVert\,d(\boldsymbol{x})\big) \,\underset{\lVert\boldsymbol{w}\rVert \to +\infty}{\approx}\, 
  \lVert\boldsymbol{w}\rVert \, \max\left(-y\,d(\boldsymbol{x}),0\right)$$
  For convenience, we name the set of misclassified data:
  $$M := \{ (\boldsymbol{x},y) \in T \;|\; y\,d(\boldsymbol{x}) \leq 0 \}$$ 
  and we can then write the empirical risk as:
  $$R(\boldsymbol{w},b) \;\underset{\lVert\boldsymbol{w}\rVert \to +\infty}{\approx}\; 
  \lVert\boldsymbol{w}\rVert \, \biggl(\frac{1}{n}\sum_{(\boldsymbol{x},y) \in M}\!\left(-\,y\,d(\boldsymbol{x})\right)\biggr)$$
  This expression contains a term which we call the <em>error distance</em>:
  $$d_{\text{err}} := \frac{1}{n}\sum_{(\boldsymbol{x},y) \in M}\!\left(-\,y\,d(\boldsymbol{x})\right)$$
  It is <em>positive</em> and can be interpreted as the average distance by which each training sample is misclassified by $\mathcal{C}$
  (with a null contribution for the correctly classified data). It is related&mdash;although not exactly equivalent&mdash;to the training error<dt-fn> 
	A small error distance $d_{\text{err}}$ does not <em>guarantee</em> the training error $err_{\text{train}}$ to be small 
    (in the worst case, when all the data lies exactly on the boundary, $d_{\text{err}} = 0$ and $err_{\text{train}} = 100\%$).</dt-fn>.</p>
  
  <p> Finally we have:
  $$\text{minimize: }R(\boldsymbol{w},b) \;\underset{\lVert\boldsymbol{w}\rVert \to +\infty}{\iff}\; \text{minimize: $d_{\text{err}}$}$$
  In words, when $\lVert\boldsymbol{w}\rVert$ is large, <em>minimizing the empirical risk for the hinge loss or the softplus loss is equivalent to minimizing the error distance, 
  which is similar to minimizing the error rate on the training set</em>.</p>
  <div class="l-body value-w">
    <div class="value-w-div">
      <p class="value-w-header"> When$\;\lVert\boldsymbol{w}\rVert\;$is small: </p>
      <p class="value-w-text"> The hinge loss and the softplus loss penalize the entire training data linearly. </p>
	</div>
	<div class="value-w-input" id="value-w-input2"> </div>
  </div>
  <p> More precisely, both losses satisfy<dt-fn>
  <u>Hinge loss</u><br>
  $$\max(1-y\,\lVert\boldsymbol{w}\rVert\,d(\boldsymbol{x}),0) \;\underset{\lVert\boldsymbol{w}\rVert\ \to 0}{=}\; 1-y\,\lVert\boldsymbol{w}\rVert\,d(\boldsymbol{x})$$
  $$\alpha = 1 \quad\text{and}\quad \beta = 1$$
  <u>Softplus loss</u><br>
  $$\ln\left(1+e^{-y\,\lVert\boldsymbol{w}\rVert\,d(\boldsymbol{x})}\right) \;\underset{\lVert\boldsymbol{w}\rVert\ \to 0}{\approx}\; 
  \ln(2)-\frac{1}{2}\,y\,\lVert\boldsymbol{w}\rVert\,d(\boldsymbol{x})$$
  $$\alpha = \ln(2) \quad\text{and}\quad \beta = \frac{1}{2}$$
  </dt-fn>:
  $$f\big(y\,\lVert\boldsymbol{w}\rVert\,d(\boldsymbol{x})\big) \,\underset{\lVert\boldsymbol{w}\rVert\ \to 0}{\approx}\, 
  \alpha - \beta\;y\,\lVert\boldsymbol{w}\rVert\,d(\boldsymbol{x})$$
  for some positive values $\alpha$ and $\beta$.</p>
  <p> We can then write the empirical risk as:
  $$R(\boldsymbol{w},b) \;\underset{\lVert\boldsymbol{w}\rVert\ \to 0}{\approx}\;
  \alpha - \beta\;\lVert\boldsymbol{w}\rVert \, \biggl(\frac{1}{n}\sum_{(\boldsymbol{x},y) \in T}y\,d(\boldsymbol{x})\biggr)$$
  This expression contains a term which we call the <em>adversarial distance</em>:
  $$d_{\text{adv}} := \frac{1}{n}\sum_{(\boldsymbol{x},y) \in T}y\,d(\boldsymbol{x})$$
  It is the mean distance between the images in $T$ and the classification boundary $\mathcal{C}$ (with a negative contribution for the misclassified images).
  It can be viewed as a measure of robustness to adversarial perturbations: when $d_{\text{adv}}$ is high, the number of misclassified images is limited and 
  the correctly classified images are far from $\mathcal{C}$.
  </p>
  <p> Finally we have:
  $$\text{minimize: }R(\boldsymbol{w},b) \;\underset{\lVert\boldsymbol{w}\rVert \to 0}{\iff}\; \text{maximize: } d_{\text{adv}}$$
  In words, when $\lVert\boldsymbol{w}\rVert$ is small, <em>minimizing the empirical risk for the hinge loss or the softplus loss is equivalent to maximizing the adversarial distance, 
  which can be interpreted as minimizing the phenomenon of adversarial examples.</em>
  
  <script>
    large_norm_w();
	small_norm_w();
  </script>
  
  <p>In practice, the value of $\lVert\boldsymbol{w}\rVert$ can be controlled by adding a regularization term to the empirical risk, yielding the <em>regularized loss</em>:
  $$L(\boldsymbol{w},b) := \underbrace{R(\boldsymbol{w},b)}_{\text{Empirical Risk}} \, + \underbrace{\lambda\,\lVert\boldsymbol{w}\rVert^2}_{\text{L2 Regularization}}$$</p>
  
  <p> A small <em>regularization parameter</em> $\lambda$ lets $\lVert\boldsymbol{w}\rVert$ grow unchecked while a larger $\lambda$ encourages $\lVert\boldsymbol{w}\rVert$ to shrink.</p>
  
  <style>
  .sum-paragraph-container {
    margin: 20px;
    background-color: rgb(95%,95%,95%);
	border-top: 15px solid rgb(95%,95%,95%);
	border-bottom: 15px solid rgb(95%,95%,95%);
	border-left: 20px solid rgb(95%,95%,95%);
	border-right: 20px solid rgb(95%,95%,95%);
	border-radius: 10px;
	font-size: 17px;
	//font-style: italic;
	text-align: center;
  }
  </style>
  
  <div>
  <div class="sum-paragraph-container">
    <i>In summary, the two standard models used in linear classification <br>(SVM and logistic regression) balance between two objectives:<br>
	they</i> <b>minimize the error distance</b> <i>when regularization is low and<br> they </i><b>maximize the adversarial distance</b> <i>when regularization is high.</i>
  </div>
  </div>
  
  <h3> Adversarial Distance and Tilting Angle </h3>
  
  <p> The adversarial distance $d_{\text{adv}}$ naturally emerged as a measure of robustness to adversarial perturbations.
  Rather conveniently, it can be expressed as a function of a single parameter.</p>

  <p>If $T_I$ and $T_J$ are the restrictions of $T$ to the elements in $I$ and $J$ respectively, we can write: 
  \begin{align}
  d_{\text{adv}} &= \frac{1}{n}\sum_{(\boldsymbol{x},y) \in T}y\,d(\boldsymbol{x})\\
				 &= \frac{1}{n}\,\biggl[\;\sum_{\boldsymbol{x} \in T_I}(-\boldsymbol{\hat{w}} \!\cdot\! \boldsymbol{x} - b^\prime) \;\;+\;\; 
				 \sum_{\boldsymbol{x} \in T_J}(\boldsymbol{\hat{w}} \!\cdot\! \boldsymbol{x} + b^\prime)\,\biggr]
  \end{align}
  If $T_I$ and $T_J$ are balanced $\left(n = 2\,n_I = 2\,n_J\right)$:
  \begin{align}
  d_{\text{adv}} &= -\frac{1}{2\,n_I}\sum_{\boldsymbol{x} \in T_I}\boldsymbol{\hat{w}} \!\cdot\! \boldsymbol{x} \;\;+\;\; \frac{1}{2\,n_J}\sum_{\boldsymbol{x} \in T_J}\boldsymbol{\hat{w}} \!\cdot\! \boldsymbol{x}\\
                 &= \frac{1}{2}\;\boldsymbol{\hat{w}} \!\cdot\! \biggl[\biggl(\frac{1}{n_J} \sum_{\boldsymbol{x} \in T_J}\boldsymbol{x}\biggr) 
				 - \biggl(\frac{1}{n_I} \sum_{\boldsymbol{x} \in T_I}\boldsymbol{x}\biggr)\biggr]
  \end{align}
  If $\boldsymbol{i}$ and $\boldsymbol{j}$ are the centroids of $T_I$ and $T_J$ respectively:
  $$d_{\text{adv}} = \frac{1}{2}\;\boldsymbol{\hat{w}} \!\cdot\! (\boldsymbol{j}-\boldsymbol{i})$$
  We now introduce the <em>nearest centroid classifier</em>, which has unit normal vector 
  $\boldsymbol{\hat{z}} = (\boldsymbol{j}-\boldsymbol{i})/\lVert\boldsymbol{j}-\boldsymbol{i}\rVert$:
  $$d_{\text{adv}} =\frac{1}{2}\;\lVert\boldsymbol{j}-\boldsymbol{i}\rVert\;\boldsymbol{\hat{w}} \!\cdot\! \boldsymbol{\hat{z}}$$
  Finally, we call the plane containing $\boldsymbol{\hat{w}}$ and $\boldsymbol{\hat{z}}$ the <em>tilting plane</em> of $\mathcal{C}$ 
  and we we call the angle $\theta$ between $\boldsymbol{\hat{w}}$ and $\boldsymbol{\hat{z}}$ the <em>tilting angle</em> of $\mathcal{C}$:
  $$d_{\text{adv}} = \frac{1}{2}\;\lVert\boldsymbol{j}-\boldsymbol{i}\rVert\;\cos(\theta)$$</p>

  <p> Hence, on a given training set $T$ where the distance between the two centroids $\lVert\boldsymbol{j}-\boldsymbol{i}\rVert$ is fixed, 
  $d_{\text{adv}}$ depends only on the tilting angle $\theta$. <br>Two observations follow:<br>
  <ul> 
  <li> The adversarial example phenomenon is minimized by the nearest centroid classifier<dt-fn>
  ... <u>and</u> the classifiers parallel to it ($d_{\text{adv}}$ is independent 
  of the bias $b$). This explains why the biases found by SVM are poorly adjusted when regularization is high, resulting in very high training and test errors 
  (see for instance the classification of 1s vs 8s when $\lambda = 10^7$ in the next section).</dt-fn> (i.e. $\theta = 0$).</li>
  <li> Adversarial examples can be arbitrarily strong when $\theta \to \pi/2$ (as was the case with the classifier $\mathscr{L}_{\theta}$ in the introductory toy problem).</li>
  </ul>
  
  <h3> Example: SVM on MNIST </h3>
  
  <p> We illustrate the previous considerations on binary classification of MNIST digits. For each possible pair of digit classes, we compute the normalized 
  weight vector $\boldsymbol{\hat{z}}$ of the nearest centroid classifier and train multiple SVM models $(\boldsymbol{w},b)$ for $\lambda \in [10^{-1}, 10^7]$
  (using a training set of $3000$ images per class).</p>
  
  <p> Then, we look at representative images $\boldsymbol{x}$, $\boldsymbol{y}$ (one per class) and, for each SVM model, their mirror images 
  $\boldsymbol{x}_m$, $\boldsymbol{y}_m$. We also look at their projections in the tilting plane of $\boldsymbol{\hat{w}}$ along with the 
  projections of the rest of the training data:</p>
  
  <style>
  #fig4 {
    display: -webkit-flex;
    display: flex;
  }
  #fig4-left {
    flex-grow: 1;
    display: -webkit-flex;
    display: flex;
	flex-direction: column;
	justify-content: flex-end;
  }
  #fig4-left-controler {
    margin-left: auto;
	margin-right: auto;
	margin-bottom: 40px;
    display: -webkit-flex;
    display: flex;
	justify-content: center;
  }
  .fig4-left-caption0 {
    font-size: 15px;
  }
  .fig4-left-caption1 {
    margin-bottom: 10px;
  }
  .fig4-left-caption2 {
	margin-bottom: 5px;
	text-align: center;
  }
  .fig4-left-caption3 {
    margin-top: auto;
	margin-bottom: 33px;
  }
  #fig4-value-lambda {
    display: inline-block;
	width: 23px;
	text-align: right;
  }
  .fig4-span {
    display: inline-block;
	width: 30px;
	text-align: right;
  }
  #fig4-left-controler-input {
    margin-top: 7px;
    margin-left: 10px;
	margin-right: 10px;
  }
  #fig4-left-line1 {
	margin-top: 5px;
	margin-bottom: auto;
	margin-right: 30px;
    display: -webkit-flex;
    display: flex;
	//justify-content: space-around;
  }
  #fig4-left-line1-errors {
    margin-right: 60px;
    display: -webkit-flex;
    display: flex;
	flex-direction: column;
	align-items: flex-end;
  }
  .fig4-left-line {
	margin-bottom: 20px;
    display: -webkit-flex;
    display: flex;
  }
  .fig4-left-cell {
    display: -webkit-flex;
    display: flex;
	flex-direction: column;
  }
  #fig4-right {
    display: -webkit-flex;
    display: flex;
	flex-direction: column;
  }
  #fig4-right-buttons {
    flex-grow: 1;
	margin-bottom: 10px;
    display: -webkit-flex;
    display: flex;
	justify-content: space-between;
	font-size: 13px;
  }
  .top-row {
    color: rgb(78.07%,51.10%,4.21%);
  }
  .bottom-row {
    color: rgb(36.84%,50.68%,70.98%);
  }
  #fig4-right-content {
	width: 430px;
	height: 280px;
  }
  #fig4-caption {
    margin-top: 10px;
  }
  </style>
  
  <figure class="l-middle">
	<div id="fig4-left-controler">
	  <div class="fig4-left-caption0"> Regularization parameter $\lambda:$ </div>
	  <input id="fig4-left-controler-input" type="range" min="0" max="80" step="1"> </input>
	  <div class="fig4-left-caption0"> $\displaystyle 10$ <sup> <span id="fig4-value-lambda">...</span> </sup> </div>
    </div>
    <div id="fig4">
	  <div id="fig4-left">
		<div id="fig4-left-line1">
		  <div id="fig4-left-line1-errors">
		    <figcaption id="fig4-left-errTrain" class="fig4-left-caption1"> $err_{\text{train}} = $ <span class="fig4-span" id="fig4-value-errTrain">...</span>% </figcaption>
			<figcaption id="fig4-left-errTest" class="fig4-left-caption1"> $err_{\text{test}} = $ <span class="fig4-span" id="fig4-value-errTest">...</span>% </figcaption>
	      </div>
		  <figcaption id="fig4-left-dAdv"> $d_{\text{adv}} =  $ <span class="fig4-span" id="fig4-value-dAdv">...</span> </figcaption>
		</div>
		<div class="fig4-left-line">
		  <div class="fig4-left-cell">
		    <figcaption class="fig4-left-caption2"> $\boldsymbol{x}$ </figcaption>
		    <canvas id="fig4-left-canvas-imx" width="90" height="90"> </canvas>
		  </div>
		  <figcaption class="fig4-left-caption3"> $\;+$ <span class="fig4-span" id="fig4-value-dx">...</span> $\,\times\,$ </figcaption>
		  <div class="fig4-left-cell">
		    <figcaption class="fig4-left-caption2"> $\boldsymbol{\hat{w}}$ </figcaption>
			<canvas id="fig4-left-canvas-w" width="90" height="90"> </canvas>
		  </div>
		  <figcaption class="fig4-left-caption3"> $\;=\;$ </figcaption>
		  <div class="fig4-left-cell">
			<figcaption class="fig4-left-caption2"> $\boldsymbol{x}_m$ </figcaption>
			<canvas id="fig4-left-canvas-imxm" width="90" height="90"> </canvas>
		  </div>
		</div>
		<div class="fig4-left-line">
		  <div class="fig4-left-cell">
		    <figcaption class="fig4-left-caption2"> $\boldsymbol{y}$ </figcaption>
		    <canvas id="fig4-left-canvas-imy" width="90" height="90"> </canvas>
		  </div>
		  <figcaption class="fig4-left-caption3"> $\;+$ <span class="fig4-span" id="fig4-value-dy">...</span> $\,\times\,$ </figcaption>
		  <div class="fig4-left-cell">
			<figcaption class="fig4-left-caption2"> $-\boldsymbol{\hat{w}}$ </figcaption>
			<canvas id="fig4-left-canvas-minus-w" width="90" height="90"> </canvas>
		  </div>
		  <figcaption class="fig4-left-caption3"> $\;=\;$ </figcaption>
		  <div class="fig4-left-cell">
			<figcaption class="fig4-left-caption2"> $\boldsymbol{y}_m$ </figcaption>
			<canvas id="fig4-left-canvas-imym" width="90" height="90"> </canvas>
		  </div>
		</div>
	  </div>
	  <div id="fig4-right">
        <div id="fig4-right-buttons">
          <div>
            <span class="top-row"> Left digit:</span> <br>
	        <span class="bottom-row"> Right digit:</span>
	      </div>
	      <div class="digit-div">
	        <input id="left-input-0" type="radio" name="left-digit" value="0"> <label class="top-row" id="left-label-0" for="0"> 0 </label></input> <br>
	        <input id="right-input-0" type="radio" name="right-digit" value="0"> <label class="bottom-row" id="right-label-0" for="0"> 0 </label></input>
	      </div>
	      <div class="digit-div">
	        <input id="left-input-1" type="radio" name="left-digit" value="1"> <label class="top-row" id="left-label-1" for="1"> 1 </label></input> <br>
	        <input id="right-input-1" type="radio" name="right-digit" value="1"> <label class="bottom-row" id="right-label-1" for="1"> 1 </label></input>
	      </div>
	      <div class="digit-div">
	        <input id="left-input-2" type="radio" name="left-digit" value="2"> <label class="top-row" id="left-label-2" for="2"> 2 </label></input> <br>
	        <input id="right-input-2" type="radio" name="right-digit" value="2"> <label class="bottom-row" id="right-label-2" for="2"> 2 </label></input>
	      </div>
	      <div class="digit-div">
	        <input id="left-input-3" type="radio" name="left-digit" value="3"> <label class="top-row" id="left-label-3" for="3"> 3 </label></input> <br>
	        <input id="right-input-3" type="radio" name="right-digit" value="3"> <label class="bottom-row" id="right-label-3" for="3"> 3 </label></input>
	      </div>
	      <div class="digit-div">
	        <input id="left-input-4" type="radio" name="left-digit" value="4"> <label class="top-row" id="left-label-4" for="4"> 4 </label></input> <br>
	        <input id="right-input-4" type="radio" name="right-digit" value="4"> <label class="bottom-row" id="right-label-4" for="4"> 4 </label></input>
	      </div>
	      <div class="digit-div">
	        <input id="left-input-5" type="radio" name="left-digit" value="5"> <label class="top-row" id="left-label-5" for="5"> 5 </label></input> <br>
	        <input id="right-input-5" type="radio" name="right-digit" value="5"> <label class="bottom-row" id="right-label-5" for="5"> 5 </label></input>
	      </div>
	      <div class="digit-div">
	        <input id="left-input-6" type="radio" name="left-digit" value="6"> <label class="top-row" id="left-label-6" for="6"> 6 </label></input> <br>
	        <input id="right-input-6" type="radio" name="right-digit" value="6"> <label class="bottom-row" id="right-label-6" for="6"> 6 </label></input>
	      </div>
	      <div class="digit-div">
	        <input id="left-input-7" type="radio" name="left-digit" value="7"> <label class="top-row" id="left-label-7" for="7"> 7 </label></input> <br>
	        <input id="right-input-7" type="radio" name="right-digit" value="7"> <label class="bottom-row" id="right-label-7" for="7"> 7 </label></input>
	      </div>
	      <div class="digit-div">
	        <input id="left-input-8" type="radio" name="left-digit" value="8"> <label class="top-row" id="left-label-8" for="8"> 8 </label></input> <br>
	        <input id="right-input-8" type="radio" name="right-digit" value="8"> <label class="bottom-row" id="right-label-8" for="8"> 8 </label></input>
	      </div>
	      <div class="digit-div">
	        <input id="left-input-9" type="radio" name="left-digit" value="9"> <label class="top-row" id="left-label-9" for="9"> 9 </label></input> <br>
	        <input id="right-input-9" type="radio" name="right-digit" value="9"> <label class="bottom-row" id="right-label-9" for="9"> 9 </label></input>
	      </div>
        </div>
        <div id="fig4-right-content">
	    </div>
	  </div>
	</div>
	<figcaption id="fig4-caption"> Binary classification with SVM on MNIST as a function of the regularization parameter $\lambda$. The training error $err_{\text{train}}$,
	the test error $err_{\text{test}}$ and the adversarial distance $d_{\text{adv}}$ are indicated in the top left corner. Below are two representative images $\boldsymbol{x}$,
	$\boldsymbol{y}$ and their mirror images $\boldsymbol{x}_m$, $\boldsymbol{y}_m$. On the right are their projections in the tilting plane of $\boldsymbol{\hat{w}}$
	along with the projections of the training data. The horizontal direction is $\boldsymbol{\hat{z}}$ (joining the two centroids) and the vertical direction is 
	$\boldsymbol{\hat{n}}$ with $\boldsymbol{n} = \boldsymbol{w} - (\boldsymbol{w} \!\cdot\! \boldsymbol{\hat{z}})\,\boldsymbol{\hat{z}}$. 
	The grey area is the SVM margin of width $\frac{2}{\lVert\boldsymbol{w}\rVert}$.</figcaption>
  </figure>
  
  <style>
  #button-0vs1 {
	font: 14px -apple-system, BlinkMacSystemFont, "Roboto", sans-serif;
	color: rgb(50%,50%,50%);
  }
  #button-7vs9 {
	font: 14px -apple-system, BlinkMacSystemFont, "Roboto", sans-serif;
	color: rgb(50%,50%,50%);
  }
  </style>
  
  <p>We can make several observations:</p>
  <ul>
    <li> Minimising the training error and maximizing the adversarial distance are conflicting goals: 
	$err_{train}$ is minimized when $\lambda$ is small and $d_{adv}$ is maximized when $\lambda$ is large.</li>
    <li> The test error is minimized for an intermediate level of regularization $\lambda_{\text{optimal}}$.
	When $\lambda \lt \lambda_{\text{optimal}}$, the classifier is <em>overfitted</em> and when $\lambda \gt \lambda_{\text{optimal}}$, the classifier is <em>underfitted</em>.</li>
	<li> The training error is minimized by tilting the classification boundary along directions of low variance in the data, effectively overfitting
	a small number of training samples. This results in a large tilting angle and gives a random aspect to the weight vector.</li>
	<li> The model is sensitive to strong adverarial examples ($||\boldsymbol{x}_m-\boldsymbol{x}|| \to 0$ and $||\boldsymbol{y}_m-\boldsymbol{y}|| \to 0$)
	when the tilting angle of $\boldsymbol{\hat{w}}$ is close to $\pi/2$. This is a symptom of strong overfitting, and whether it occurs or not depends on the difficulty 
	to separate the two classes (compare for instance <button id="button-7vs9"> 7 vs 9 </button> and <button id="button-0vs1"> 0 vs 1 </button>). </li>
  </ul>
  
  <script>
   fig4();
  </script>
  
  <hr>
  
  <h2> In Neural Networks </h2>
  
  <p> Thanks to the equivalence between adversarial distance and tilting angle, the linear case is simple enough to be visualized in the plane. In neural networks however, 
  the class boundary is not flat and the adversarial distance cannot be reduced to a single parameter. Nonetheless, some similarities with the linear case remain.</p>
  
  <h3> First Step: a 2-Layer Binary Network </h3>
  
  <p> Let $\mathcal{N}$ be a 2-layer network with a single output defining a nonlinear binary classifier in $\mathbb{R}^d$. The first layer of $\mathcal{N}$ is 
  specified by a weight matrix $W_1$ and a bias vector $b_1$ and the second layer of $\mathcal{N}$ is specified by a weight vector $W_2$ and bias $b_2$. We assume
  that the two layers are separated by a layer $\phi$ of rectified linear units applying the function $z \rightarrow \max(0,z)$ element-wise.   
  For an image $x$ in $\mathbb{R}^d$, we call the <em>raw score</em> of $x$ through $\mathcal{N}$ the value:
  $$s(x) := W_2\;\phi(W_1\,x + b_1) + b_2$$ 
  Similarly to the linear case, the <em>empirical risk</em> on $T$ for a <em>loss function</em> $f$ can be written:
  $$R(W_1,b_1;W_2,b_2) := \frac{1}{n}\sum_{(x,y) \in T}f\big(y\,s(x)\big)$$
  and training $\mathcal{N}$ consists in finding $W_1$, $b_1$, $W_2$ and $b_2$ minimizing $R$ for a well chosen $f$.</p> 
  
  <p> $\phi$ is piecewise linear and around each image $x$ there is a local linear region $\mathcal{L}_x$ within which:
  $$\phi(W_1 \, x + b_1) = W_1^x \, x + b_1^x$$
  where $W_1^x$ and $b_1^x$ are obtained by zeroing some lines in $W_1$ and $b_1$ respectively<dt-fn>
  More precisely, the $i^{th}$ lines in $W_1^x$ and $b_1^x$ are:
  $$\left(W_1^x\,,\,b_1^x\right)_i = 
  \begin{cases}
    \left(W_1\,,\,b_1\right)_i  \quad & \text{if } \left(W_1\,x + b_1\right)_i \gt 0\\
    0                                 & \text{if } \left(W_1\,x + b_1\right)_i \leq 0
  \end{cases}$$
  </dt-fn>. Within $\mathcal{L}_x$, the raw score can thus be written:
  $$s(x) = W_2 W_1^x \, x + W_2 b_1^x + b_2$$
  and can be seen as the raw score of a local linear classifier $\mathcal{C}_x$.
  Our analysis of the linear case then applies almost without modifications. First, we observe that $s(x)$ is a scaled distance. 
  If $d(x)$ is the actual <em>signed euclidean distance</em> between $x$ and $\mathcal{C}_x$, we have:
  $$s(x) = \lVert W_2 W_1^x \rVert\,d(x)$$</p>
  
  <style>
  #notes-div {
    display: -webkit-flex;
    display: flex;
  }
  #notes-p {
    margin-top: 0px;
	margin-bottom: 0px;
	margin-left: 20px;
  }
  </style>
  
  <div id="notes-div">
  <div> Notes </div>
  <p id="notes-p"> &#9656; $d(x)$ can also be viewed as a linear approximation of the distance between $x$ and the boundary defined by 
  $\mathcal{N}$ (that is, the distance to the nearest adversarial example).<br> 
  &#9656; $W_2 W_1^x$ is the gradient of $\mathcal{N}$ within $\mathcal{L}_x$. 
  It is the adversarial direction for $x$, computed by backpropagation in practice.</p>
  </div>
  
  <p> The norm $\lVert W_2 W_1^x \rVert$ can then be interpreted as a scaling parameter for the loss function (the scaling is now local, dependent on $x$).
  One simple way to control all the local scalings simultaneously is by adding a regularization term to the empirical risk acting on the norms 
  $\lVert W_1 \rVert$ and $\lVert W_2 \rVert$ independently (remember that the weights in $W_1^x$ are a subset of the weights in $W_1$). 
  With gradient descent, this ends up being equivalent to decaying the weights $W_1$ and $W_2$ at every iteration.
  More precisely, for a learning rate $\eta$ and a decaying factor $\lambda$, the <em>weight decay update</em> is:
  $$W_1 \leftarrow W_1 - \eta\,\lambda\,W_1 \quad \text{and} \quad W_2 \leftarrow W_2 - \eta\,\lambda\,W_2$$
  
  <style>
  .weight-decay-header {
    margin-top: 0px;
	margin-bottom: 0px;
    font: 18px -apple-system, BlinkMacSystemFont, "Roboto", sans-serif;
	font-weight: bold;
  }
  </style>
  
  <ul> 
  <li> <span class="weight-decay-header"> With a small decaying factor$\;\lambda$, </span> 
  the scaling parameter $\lVert W_2 W_1^x \rVert$ is allowed to grow unrestricted and the loss penalizes only 
  the misclassified data. Minimizing the empirical risk is then equivalent to minimizing the error on the training set.
  </li>
  <li> <span class="weight-decay-header"> As the decaying factor$\;\lambda\;$increases, </span>
  the scaling parameter $\lVert W_2 W_1^x \rVert$ decreases and the loss starts penalizing more and more of the 
  correctly classified data, pushing it further away from the boundary. Under this light, L2 weight decay can be seen as a form of <em>adversarial training</em>.
  </li>
  </ul>
  
  <div>
  <div class="sum-paragraph-container">
  In summary, a 2-layer network differs from a linear classifier in <br> that the distances on which the loss function is applied are<br> <u>linear approximations</u> of the 
  actual distances between the data and<br> the boundary and the scaling parameter $\lVert W_2 W_1^x \rVert$ is <u>local</u>.
  </div>
  </div>
  
  <h3> Second Step: General Case </h3>
  
  <p> The previous analysis can be generalized to more layers and even to non-piecewise-linear activation functions. The important observation is that we always have:
  $$s(x) = \lVert \nabla_{\!x} \, s \rVert\, d(x)$$
  Where $\nabla_{\!x} \, s$ is the gradient of the raw score on $x$, and $d(x)$ is a linear approximation of the distance between $x$ and the boundary defined by the network.
  The norm $\lVert \nabla_{\!x} \, s \rVert$ then acts as a scaling parameter on the loss function.</p> 
  
  <p> Another point that needs to be addressed is that, until now, we have only focussed on binary classification using the hinge loss or the softplus loss. 
  This might appear as a serious restriction, but it's not. In the multiclass case, the raw score simply becomes a vector and each of its components 
  can be seen as a scaled distance. The multiclass loss then applied is&mdash;in most problems&mdash;the cross-entropy loss, which is nothing more than a 
  multidimensional version of the softplus loss.</p>
  
  <p>The fact that deep networks generally converge to zero training error in practice and the fact that they are often
  vulnerable to linear attacks of small magnitude <dt-cite key="goodfellow2014explaining"></dt-cite> suggests that they are strongly under-regularized.</p>

  <h3> Example: LeNet on MNIST </h3>
  
  <p> Is it possible to regularize a neural network against adversarial examples by only using weight decay? The idea is simple enough and has been considered before:
  Goodfellow et al. <dt-cite key="goodfellow2014explaining"></dt-cite> have observed that adversarial training is <em>"somewhat similar to L1 regularization"</em> in 
  the linear case. However, the authors reported that when training maxout networks on MNIST, an L1 weight decay coefficient of $0.0025$ <em>"was too large, and caused 
  the model to get stuck with over $5\%$ error on the training set. Smaller weight decay coefficients permitted successful training but conferred no regularization benefit."</em><p>
  
  <p> We put the idea to the test once again in a simple setup: LeNet on MNIST (10-class problem). As reported in <dt-cite key="goodfellow2014explaining"></dt-cite>, 
  we initially observed a high training error when using large L2 weight decay, but then found that it could be fixed by disabling momentum (momentum gives a short term
  memory to the gradient descent algorithm <dt-cite key="goh2017why"></dt-cite>). Further investigation showed that momentum could remain active, as long 
  as it applied only to the gradient step (and not the decay step). With this modification, we were able to train LeNet on MNIST using a weight decay of $1.0$ on every layer
  ($50$ epochs, learning rate of $0.0005$). For comparison, we also trained the same network with a standard weight decay of $0.0005$. </p>
  
  <p> Below, we submit the two networks to the same evaluation and generate high-confidence adversarial examples (with median confidence scores) targeted to perform a 
  cyclic permutation of the labels: $0 \to 1$, ..., $9 \to 0$. For convenience, we set the temperature of the softmax layer on each network<dt-fn>
  If the inputs to the softmax layer are the raw scores $s_i$ and the outputs are the corresponding probabilities $p_i$, then:
  $$p_i = \frac{\exp(s_i/T)}{\displaystyle \sum_j \exp(s_j/T)}$$
  where T is the temperature parameter. It was introduced in the context of network distillation and produces softer probability distributions over classes <dt-cite key="hinton2015distilling"></dt-cite>.</dt-fn>
  to ensure that the median score on the test set is 0.95.</p>
  
  <style>
  #fig5 {
    display: -webkit-flex;
    display: flex;
	flex-direction: column;
  }
  .fig5-line {
    margin-bottom: 20px;
    display: -webkit-flex;
    display: flex;
  }
  .fig5-header {
    margin-bottom: 10px;
	font-size: 15px;
	text-align: center;
  }
  .fig5-column1 {
    margin-right: 20px;
    display: -webkit-flex;
    display: flex;
	flex-direction: column;
	width: 90px;
  }
  .fig5-caption1 {
    margin-top: 18px;
	text-align: center;
  }
  .fig5-caption2 {
    margin-top: 22px;
	text-align: center;
  }
  .fig5-caption3 {
    margin-top: 30px;
	text-align: center;
  }
  #fig5-img1 {
    width:500px;
	height:180px;
  }
  #fig5-img2 {
    margin-left: auto;
	margin-right: 0px;
    width:175px;
	height:184px;
  }
  </style>
  
  <figure class="l-middle">
    <div id="fig5">
	<div class="fig5-header"> Standard weight decay = 0.0005</div>
	<div class="fig5-line">
	  <div class="fig5-column1">
	    <figcaption class="fig5-caption1"> original <br> images</figcaption>
		<figcaption class="fig5-caption2"> adversarial examples</figcaption>
		<figcaption class="fig5-caption3"> perturbations</figcaption>
	  </div>
	  <img id="fig5-img1" src="assets/adv_weightDecay0.0005.png">
      <img id="fig5-img2" src="assets/plot0.0005.png">
	</div>
	<div class="fig5-header"> High weight decay = 1.0</div>
	<div class="fig5-line">
	  <div class="fig5-column1">
	    <figcaption class="fig5-caption1"> original <br> images</figcaption>
		<figcaption class="fig5-caption2"> adversarial examples</figcaption>
		<figcaption class="fig5-caption3"> perturbations</figcaption>
	  </div>
	  <img id="fig5-img1" src="assets/adv_weightDecay1.png">
      <img id="fig5-img2" src="assets/plot1.0.png">
	</div>
	</div>
	<figcaption> LeNet on MNIST trained with two different levels of weight decay. For each network, the first line shows one digit per class (tag: prediction@score),
	the second line shows the corresponding adversarial example (tag: prediction@score) and the third line shows the corresponding perturbation (tag: L2 norm).
    On the right are the train and test errors as a function of the epoch number.</figcaption>
  </figure>
  
  <p>The network trained with high weight decay is superior to the comparison network in two ways:</p>
  <ul> 
  <li> It is less overfitted (the train and test errors are approximately equal at the end of the training) 
  and generalizes better (final test error of $1.2\%$ with high weight decay versus $1.5\%$ with low weight decay). </li>
  <li> It is less vulnerable to adversarial perturbations (the perturbations have higher L2 norms and are more meaningful for human observers). </li>
  </ul>
  <hr>
  
  <h2>Thoughts Moving Forward</h2>
  
  <p> Despite the widespread interest it has generated for several years now, and despite its significance for the field of machine learning both in theory and in practice, 
  the adversarial example phenomenon has so far retained much of its intrigue. Our main goal here was to provide a clear and intuitive picture of the phenomenon 
  in the linear case, hopefully constituting a solid base from which to move forward. We also showed that our approach generalizes to some extent 
  to nonlinear neural network classifiers.</p>
  
  <p> The result with LeNet on MNIST is encouraging&mdash;but it raises more questions than it answers.
  For instance, can deeper models be regularized in the same way on more sophisticated datasets (our experiments have been inconclusive so far)? 
  Why did momentum interfere with weight decay in the experiment? Are regularized solutions harder to converge to? 
  Is weight decay the right way to control the scaling of the loss function or should the raw score be normalized in some way? 
  Is it reasonable to regard the raw score as a linear approximation of the distance between the image and the boundary for deeper models?
  In addressing these questions, one should also keep in mind that it is possible&mdash;likely, even&mdash;that a satisfying solution to the problem
  will require profoundly new ideas in deep learning.</p>

  <p> The study of the adversarial example phenomenon is a very active research area. For instance, a set of NIPS competitions hosted on Kaggle currently challenges 
  the community to come up with new attacks
  (<a href="https://www.kaggle.com/c/nips-2017-targeted-adversarial-attack">targeted</a> and
  <a href="https://www.kaggle.com/c/nips-2017-non-targeted-adversarial-attack">non-targeted</a>) and
  <a href="https://www.kaggle.com/c/nips-2017-defense-against-adversarial-attack">new defenses</a>.
  Interested readers might also want to visit Ian Goodfellow's and Nicolas Papernot's <a href="http://www.cleverhans.io/">cleverhans blog</a>,
  dedicated to the topic.</p>

<dt-appendix>
</dt-appendix>

<script type="text/bibliography">
@article{szegedy2013intriguing,
  title={Intriguing properties of neural networks},
  author={Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian and Fergus, Rob},
  journal={arXiv preprint arXiv:1312.6199},
  year={2013},
  url={https://arxiv.org/pdf/1312.6199.pdf}
}
@article{goodfellow2014explaining,
  title={Explaining and harnessing adversarial examples},
  author={Goodfellow, Ian J and Shlens, Jonathon and Szegedy, Christian},
  journal={arXiv preprint arXiv:1412.6572},
  year={2014},
  url={https://arxiv.org/pdf/1412.6572.pdf}
}
@article{kurakin2016adversarial,
  title={Adversarial examples in the physical world},
  author={Kurakin, Alexey and Goodfellow, Ian and Bengio, Samy},
  journal={arXiv preprint arXiv:1607.02533},
  year={2016},
  url={https://arxiv.org/pdf/1607.02533.pdf}
}
@article{papernot2016practical,
  title={Practical black-box attacks against deep learning systems using adversarial examples},
  author={Papernot, Nicolas and McDaniel, Patrick and Goodfellow, Ian and Jha, Somesh and Celik, Z Berkay and Swami, Ananthram},
  journal={arXiv preprint arXiv:1602.02697},
  year={2016},
  url={https://arxiv.org/pdf/1412.6572.pdf}
}
@inproceedings{moosavi2016deepfool,
  title={Deepfool: a simple and accurate method to fool deep neural networks},
  author={Moosavi-Dezfooli, Seyed-Mohsen and Fawzi, Alhussein and Frossard, Pascal},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={2574--2582},
  year={2016},
  url={https://arxiv.org/pdf/1511.04599.pdf}
}
@article{carlini2016towards,
  title={Towards evaluating the robustness of neural networks},
  author={Carlini, Nicholas and Wagner, David},
  journal={arXiv preprint arXiv:1608.04644},
  year={2016},
  url={https://arxiv.org/pdf/1608.04644.pdf}
}
@inproceedings{bastani2016measuring,
  title={Measuring neural net robustness with constraints},
  author={Bastani, Osbert and Ioannou, Yani and Lampropoulos, Leonidas and Vytiniotis, Dimitrios and Nori, Aditya and Criminisi, Antonio},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2613--2621},
  year={2016},
  url={https://arxiv.org/pdf/1605.07262.pdf}
}
@inproceedings{fawzi2016robustness,
  title={Robustness of classifiers: from adversarial to random noise},
  author={Fawzi, Alhussein and Moosavi-Dezfooli, Seyed-Mohsen and Frossard, Pascal},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1632--1640},
  year={2016},
  url={https://arxiv.org/pdf/1608.08967.pdf}
}
@article{gu2014towards,
  title={Towards deep neural network architectures robust to adversarial examples},
  author={Gu, Shixiang and Rigazio, Luca},
  journal={arXiv preprint arXiv:1412.5068},
  year={2014},
  url={https://arxiv.org/pdf/1412.5068.pdf}
}
@inproceedings{papernot2016distillation,
  title={Distillation as a defense to adversarial perturbations against deep neural networks},
  author={Papernot, Nicolas and McDaniel, Patrick and Wu, Xi and Jha, Somesh and Swami, Ananthram},
  booktitle={Security and Privacy (SP), 2016 IEEE Symposium on},
  pages={582--597},
  year={2016},
  organization={IEEE},
  url={https://arxiv.org/pdf/1511.04508.pdf}
}
@article{zhao2016suppressing,
  title={Suppressing the Unusual: towards Robust CNNs using Symmetric Activation Functions},
  author={Zhao, Qiyang and Griffin, Lewis D},
  journal={arXiv preprint arXiv:1603.05145},
  year={2016},
  url={https://arxiv.org/pdf/1603.05145.pdf}
}
@article{bhagoji2017dimensionality,
  title={Dimensionality Reduction as a Defense against Evasion Attacks on Machine Learning Classifiers},
  author={Bhagoji, Arjun Nitin and Cullina, Daniel and Mittal, Prateek},
  journal={arXiv preprint arXiv:1704.02654},
  year={2017},
  url={https://arxiv.org/pdf/1704.02654.pdf}
}
@article{feinman2017detecting,
  title={Detecting Adversarial Samples from Artifacts},
  author={Feinman, Reuben and Curtin, Ryan R and Shintre, Saurabh and Gardner, Andrew B},
  journal={arXiv preprint arXiv:1703.00410},
  year={2017},
  url={https://arxiv.org/pdf/1703.00410.pdf}
}
@article{grosse2017statistical,
  title={On the (statistical) detection of adversarial examples},
  author={Grosse, Kathrin and Manoharan, Praveen and Papernot, Nicolas and Backes, Michael and McDaniel, Patrick},
  journal={arXiv preprint arXiv:1702.06280},
  year={2017},
  url={https://arxiv.org/pdf/1702.06280.pdf}
}
@article{metzen2017detecting,
  title={On detecting adversarial perturbations},
  author={Metzen, Jan Hendrik and Genewein, Tim and Fischer, Volker and Bischoff, Bastian},
  journal={arXiv preprint arXiv:1702.04267},
  year={2017},
  url={https://arxiv.org/pdf/1702.04267.pdf}
}
@article{shaham2015understanding,
  title={Understanding Adversarial Training: Increasing Local Stability of Neural Nets through Robust Optimization},
  author={Shaham, Uri and Yamada, Yutaro and Negahban, Sahand},
  journal={arXiv preprint arXiv:1511.05432},
  year={2015},
  url={https://arxiv.org/pdf/1511.05432.pdf}
}
@article{miyato2017virtual,
  title={Virtual Adversarial Training: a Regularization Method for Supervised and Semi-supervised Learning},
  author={Miyato, Takeru and Maeda, Shin-ichi and Koyama, Masanori and Ishii, Shin},
  journal={arXiv preprint arXiv:1704.03976},
  year={2017},
  url={https://arxiv.org/pdf/1704.03976.pdf}
}
@article{sankaranarayanan2017regularizing,
  title={Regularizing deep networks using efficient layerwise adversarial training},
  author={Sankaranarayanan, Swami and Jain, Arpit and Chellappa, Rama and Lim, Ser Nam},
  journal={arXiv preprint arXiv:1705.07819},
  year={2017},
  url={https://arxiv.org/pdf/1705.07819.pdf}
}

@article{carlini2017adversarial,
  title={Adversarial Examples Are Not Easily Detected: Bypassing Ten Detection Methods},
  author={Carlini, Nicholas and Wagner, David},
  journal={arXiv preprint arXiv:1705.07263},
  year={2017},
  url={https://arxiv.org/pdf/1602.02697.pdf}
}
@misc{goodfellow2017attacking,
  title = {Attacking Machine Learning with Adversarial Examples},
  author = {Goodfellow, Ian and Papernot, Nicolas and Huang, Sandy and Duan, Yan and Abbeel, Pieter and Clark, Jack},
  year = {2017},
  url={https://blog.openai.com/adversarial-example-research}
}
@article{carlini2017adversarial,
  title={Adversarial Examples Are Not Easily Detected: Bypassing Ten Detection Methods},
  author={Carlini, Nicholas and Wagner, David},
  journal={arXiv preprint arXiv:1705.07263},
  year={2017},
  url={https://arxiv.org/pdf/1705.07263.pdf}
}
@inproceedings{parkhi2015deep,
  title={Deep Face Recognition.},
  author={Parkhi, Omkar M and Vedaldi, Andrea and Zisserman, Andrew and others},
  booktitle={BMVC},
  volume={1},
  number={3},
  pages={6},
  year={2015},
  url={http://www.robots.ox.ac.uk:5000/~vgg/publications/2015/Parkhi15/parkhi15.pdf}
}
@article{goh2017why,
  author = {Goh, Gabriel},
  title = {Why Momentum Really Works},
  journal = {Distill},
  year = {2017},
  url = {http://distill.pub/2017/momentum},
  doi = {10.23915/distill.00006}
}
@article{hinton2015distilling,
  title={Distilling the knowledge in a neural network},
  author={Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  journal={arXiv preprint arXiv:1503.02531},
  year={2015},
  url = {https://arxiv.org/pdf/1503.02531.pdf}
}
</script>

</body>
</html>
